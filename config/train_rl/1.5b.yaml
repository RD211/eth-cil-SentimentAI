train:
  number_of_problems_per_batch: 32
  num_samples_per_problem: 4

  gradient_checkpointing: true
  lr_scheduler_type: "constant"
  optimizer: "paged_adamw_8bit"
  epochs: 1
  max_steps: -1
  deepspeed_config_path: null

  beta: 0.0
  learning_rate: 2e-6

  mu: 1
  epsilon: 0.2
  sync_ref_model: false
  ref_model_mixup_alpha: 0.9
  ref_model_sync_steps: 64

model:
  model_name_or_path: "Qwen/Qwen2.5-1.5B-Instruct"
  max_length: 500
  lora:
    enable: false
    rank: 64
    alpha: 128
    target_modules: "all-linear"
    dropout: 0.01
    bias: "none"

dataset:
  datasets:
    - name_or_path: "./data/train"
      split: "train"
      ratio: 1.0
  max_examples: null
  skip: 0

huggingface:
  name: "eth-text-classification/qwen2.5-1.5b-instruct"
  push_to_hub: false

logging:
  wandb: true
  wandb_project: "train-rl"
  wandb_run_name: "1.5b-rl"
  wandb_entity: "rd211"
  run_group: "1.5b-rl"
  wandb_tags: []
  save_dir: "checkpoints/1.5b-rl"
  save_steps: 10

generation:
  prompt_template_path: "prompt_templates/sentiment.txt"
  temperature: 1.0
  top_k: 32000
  top_p: 1.0

seed: 42
