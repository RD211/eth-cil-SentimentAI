{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import Dataset\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df = pd.read_csv('data/test.csv')\n",
    "train_df = pd.read_csv('data/training.csv')\n",
    "train_df, test_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = Dataset.from_dict({'text': train_df['sentence'], 'label': train_df['label']})\n",
    "test_dataset = Dataset.from_dict({'id': test_df['id'], 'text': test_df['sentence']})\n",
    "# shuffle\n",
    "train_dataset = train_dataset.shuffle()\n",
    "\n",
    "validation_dataset = train_dataset.train_test_split(test_size=0.1)\n",
    "train_dataset = validation_dataset['train']\n",
    "validation_dataset = validation_dataset['test']\n",
    "# Save the datasets to disk\n",
    "train_dataset.save_to_disk('data/train')\n",
    "test_dataset.save_to_disk('data/test')\n",
    "validation_dataset.save_to_disk('data/validation')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"Qwen/Qwen2.5-0.5B-Instruct\")\n",
    "\n",
    "train_dataset = train_dataset.map(\n",
    "    lambda examples: {\"len\":len(tokenizer.encode(\n",
    "        examples['text'],\n",
    "    )),\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get max length\n",
    "max_length = max(train_dataset['len'])\n",
    "print(max_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from vllm import LLM, SamplingParams\n",
    "\n",
    "llm = LLM(model=\"checkpoints/0.5b-rl/checkpoint-620\", max_model_len=1500, gpu_memory_utilization=0.8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of LlamaForSequenceClassification were not initialized from the model checkpoint at HuggingFaceTB/SmolLM2-360M and are newly initialized: ['score.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LlamaForSequenceClassification(\n",
      "  (model): LlamaModel(\n",
      "    (embed_tokens): Embedding(49152, 960)\n",
      "    (layers): ModuleList(\n",
      "      (0-31): 32 x LlamaDecoderLayer(\n",
      "        (self_attn): LlamaAttention(\n",
      "          (q_proj): Linear(in_features=960, out_features=960, bias=False)\n",
      "          (k_proj): Linear(in_features=960, out_features=320, bias=False)\n",
      "          (v_proj): Linear(in_features=960, out_features=320, bias=False)\n",
      "          (o_proj): Linear(in_features=960, out_features=960, bias=False)\n",
      "        )\n",
      "        (mlp): LlamaMLP(\n",
      "          (gate_proj): Linear(in_features=960, out_features=2560, bias=False)\n",
      "          (up_proj): Linear(in_features=960, out_features=2560, bias=False)\n",
      "          (down_proj): Linear(in_features=2560, out_features=960, bias=False)\n",
      "          (act_fn): SiLU()\n",
      "        )\n",
      "        (input_layernorm): LlamaRMSNorm((960,), eps=1e-05)\n",
      "        (post_attention_layernorm): LlamaRMSNorm((960,), eps=1e-05)\n",
      "      )\n",
      "    )\n",
      "    (norm): LlamaRMSNorm((960,), eps=1e-05)\n",
      "    (rotary_emb): LlamaRotaryEmbedding()\n",
      "  )\n",
      "  (score): Linear(in_features=960, out_features=3, bias=False)\n",
      ")\n",
      "Loading llm weights for head initialization.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Linear(in_features=960, out_features=3, bias=False)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoModelForSequenceClassification, AutoTokenizer\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\n",
    "    \"HuggingFaceTB/SmolLM2-360M\",\n",
    "    padding_side=\"left\",\n",
    "    use_fast=True,\n",
    ")\n",
    "model = AutoModelForSequenceClassification.from_pretrained(\n",
    "    \"HuggingFaceTB/SmolLM2-360M\",\n",
    "    num_labels=3,\n",
    ").to(\"cuda\")\n",
    "model.config.use_cache = False\n",
    "model.config.pad_token_id = tokenizer.pad_token_id\n",
    "print(model)\n",
    "print(\"Loading llm weights for head initialization.\")\n",
    "llm = AutoModelForCausalLM.from_pretrained(\n",
    "    \"HuggingFaceTB/SmolLM2-360M\",\n",
    "    torch_dtype=torch.bfloat16,\n",
    ").to(\"cuda\")\n",
    "label2id = {\n",
    "    \"negative\": 0,\n",
    "    \"neutral\": 1,\n",
    "    \"positive\": 2\n",
    "}\n",
    "classes = list(label2id.keys())\n",
    "tokens = [tokenizer.encode(c)[-1] for c in classes]\n",
    "\n",
    "model.score.weight = nn.Parameter(llm.lm_head.weight[tokens].clone())\n",
    "# print(\"Updated weights of model with lm_head.\")\n",
    "# del llm\n",
    "model.score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`torch.nn.functional.scaled_dot_product_attention` does not support `output_attentions=True`. Falling back to eager attention. This warning can be removed using the argument `attn_implementation=\"eager\"` when loading the model.\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "expected mat1 and mat2 to have the same dtype, but got: float != c10::BFloat16",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[21], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43minput_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtensor\u001b[49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\u001b[43mtokenizer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mencode\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mI love you\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcuda\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      2\u001b[0m \u001b[43m      \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtensor\u001b[49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m10\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcuda\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      3\u001b[0m \u001b[43m      \u001b[49m\u001b[43mlabels\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtensor\u001b[49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcuda\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      4\u001b[0m \u001b[43m      \u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m      5\u001b[0m \u001b[43m      \u001b[49m\u001b[43moutput_hidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m      6\u001b[0m \u001b[43m      \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m      7\u001b[0m \u001b[43m      \u001b[49m\u001b[43muse_cache\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m      8\u001b[0m \u001b[43m      \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.12/site-packages/torch/nn/modules/module.py:1739\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1737\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1738\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1739\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.12/site-packages/torch/nn/modules/module.py:1750\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1745\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1746\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1747\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1748\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1749\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1750\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1752\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1753\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.12/site-packages/transformers/utils/generic.py:965\u001b[0m, in \u001b[0;36mcan_return_tuple.<locals>.wrapper\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    962\u001b[0m     set_attribute_for_modules(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_is_top_level_module\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[1;32m    964\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 965\u001b[0m     output \u001b[38;5;241m=\u001b[39m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    966\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m is_requested_to_return_tuple \u001b[38;5;129;01mor\u001b[39;00m (is_configured_to_return_tuple \u001b[38;5;129;01mand\u001b[39;00m is_top_level_module):\n\u001b[1;32m    967\u001b[0m         output \u001b[38;5;241m=\u001b[39m output\u001b[38;5;241m.\u001b[39mto_tuple()\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.12/site-packages/transformers/models/llama/modeling_llama.py:915\u001b[0m, in \u001b[0;36mLlamaForSequenceClassification.forward\u001b[0;34m(self, input_ids, attention_mask, position_ids, past_key_values, inputs_embeds, labels, use_cache, output_attentions, output_hidden_states)\u001b[0m\n\u001b[1;32m    904\u001b[0m transformer_outputs: BaseModelOutputWithPast \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel(\n\u001b[1;32m    905\u001b[0m     input_ids,\n\u001b[1;32m    906\u001b[0m     attention_mask\u001b[38;5;241m=\u001b[39mattention_mask,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    912\u001b[0m     output_hidden_states\u001b[38;5;241m=\u001b[39moutput_hidden_states,\n\u001b[1;32m    913\u001b[0m )\n\u001b[1;32m    914\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m transformer_outputs\u001b[38;5;241m.\u001b[39mlast_hidden_state\n\u001b[0;32m--> 915\u001b[0m logits \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mscore\u001b[49m\u001b[43m(\u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    917\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m input_ids \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    918\u001b[0m     batch_size \u001b[38;5;241m=\u001b[39m input_ids\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m]\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.12/site-packages/torch/nn/modules/module.py:1739\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1737\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1738\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1739\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.12/site-packages/torch/nn/modules/module.py:1750\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1745\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1746\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1747\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1748\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1749\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1750\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1752\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1753\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.12/site-packages/torch/nn/modules/linear.py:125\u001b[0m, in \u001b[0;36mLinear.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    124\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[0;32m--> 125\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlinear\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: expected mat1 and mat2 to have the same dtype, but got: float != c10::BFloat16"
     ]
    }
   ],
   "source": [
    "model(input_ids=torch.tensor([tokenizer.encode(\"I love you\")]).to(\"cuda\"),\n",
    "      attention_mask=torch.tensor([[1] * 10]).to(\"cuda\"),\n",
    "      labels=torch.tensor([0]).to(\"cuda\"),\n",
    "      return_dict=True,\n",
    "      output_hidden_states=True,\n",
    "      output_attentions=True,\n",
    "      use_cache=False,\n",
    "      )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_from_disk\n",
    "val_dataset = load_from_disk('data/validation')\n",
    "test_dataset = load_from_disk('data/test')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sampling_params = SamplingParams(\n",
    "    n = 1,\n",
    "    temperature=1.0,\n",
    "    top_p=0.9,\n",
    "    max_tokens=1500,\n",
    ")\n",
    "tokenizer = llm.get_tokenizer()\n",
    "prompt = open(\"prompt_templates/sentiment.txt\", \"r\").read()\n",
    "val_dataset = val_dataset.map(lambda conversation: {\"conv\": tokenizer.apply_chat_template([\n",
    "    {\"role\": \"user\", \"content\": prompt.format(conversation['text'])}\n",
    "    ], tokenize=False, add_generation_prompt=True)\n",
    "}\n",
    ")\n",
    "\n",
    "to_process = val_dataset['conv']\n",
    "outputs = llm.generate(\n",
    "    to_process,\n",
    "    sampling_params=sampling_params,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "verdicts = []\n",
    "for i in range(len(outputs)):\n",
    "  type_of_verdicts = ['boxed{negative}', 'boxed{positive}', 'boxed{neutral}']\n",
    "  texts = [outputs[i].outputs[j].text for j in range(len(outputs[i].outputs))]\n",
    "  # We do a majority vote to get the final verdict\n",
    "  verdict = max(type_of_verdicts, key=lambda x: sum([1 for text in texts if x in text]))\n",
    "  verdicts.append(verdict.replace('boxed{', '').replace('}', ''))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "def score(label, verdict):\n",
    "    if label == 'positive':\n",
    "        if verdict == 'positive':\n",
    "            return 1\n",
    "        elif verdict == 'neutral':\n",
    "            return 0.5\n",
    "        else:\n",
    "            return 0\n",
    "    elif label == 'negative':\n",
    "        if verdict == 'negative':\n",
    "            return 1\n",
    "        elif verdict == 'neutral':\n",
    "            return 0.5\n",
    "        else:\n",
    "            return 0\n",
    "    else:\n",
    "        if verdict == 'neutral':\n",
    "            return 1\n",
    "        elif verdict == 'positive':\n",
    "            return 0.5\n",
    "        else:\n",
    "            return 0.5\n",
    "scores = []\n",
    "labels = val_dataset['label']\n",
    "for i in range(len(labels)):\n",
    "    scores.append(score(labels[i], verdicts[i]))\n",
    "# get the mean of the scores\n",
    "mean_score = np.mean(scores)\n",
    "print(mean_score, np.std(scores))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sampling_params = SamplingParams(\n",
    "    n = 1,\n",
    "    temperature=1.0,\n",
    "    top_p=0.9,\n",
    "    max_tokens=1500,\n",
    ")\n",
    "tokenizer = llm.get_tokenizer()\n",
    "prompt = open(\"prompt_templates/sentiment.txt\", \"r\").read()\n",
    "test_dataset = test_dataset.map(lambda conversation: {\"conv\": tokenizer.apply_chat_template([\n",
    "    {\"role\": \"user\", \"content\": prompt.format(conversation['text'])}\n",
    "    ], tokenize=False, add_generation_prompt=True)\n",
    "}\n",
    ")\n",
    "\n",
    "to_process = test_dataset['conv']\n",
    "outputs = llm.generate(\n",
    "    to_process,\n",
    "    sampling_params=sampling_params,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "verdicts = []\n",
    "for i in range(len(outputs)):\n",
    "  type_of_verdicts = ['boxed{negative}', 'boxed{positive}', 'boxed{neutral}']\n",
    "  texts = [outputs[i].outputs[j].text for j in range(len(outputs[i].outputs))]\n",
    "  # We do a majority vote to get the final verdict\n",
    "  verdict = max(type_of_verdicts, key=lambda x: sum([1 for text in texts if x in text]))\n",
    "  verdicts.append(verdict.replace('boxed{', '').replace('}', ''))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save verdicts to file\n",
    "import pandas as pd\n",
    "submission_df = pd.DataFrame({'id': test_dataset['id'], 'label': verdicts})\n",
    "submission_df.to_csv('submission.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "import tqdm\n",
    "\n",
    "model = AutoModelForSequenceClassification.from_pretrained(\"rd211/custom-trainer\", torch_dtype=torch.bfloat16, device_map=\"auto\", num_labels=3)\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"rd211/custom-trainer\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from hydra import compose, initialize\n",
    "from omegaconf import OmegaConf\n",
    "from data_loader import get_dataset\n",
    "\n",
    "with initialize(version_base=None, config_path=\"config/classifier\", job_name=\"train\"):\n",
    "    cfg = compose(config_name=\"train\")\n",
    "\n",
    "\n",
    "cfg.data.path = './data/validation'\n",
    "ds_val, collator = get_dataset(cfg, tokenizer=tokenizer)\n",
    "\n",
    "batch_size = 1\n",
    "ds_val = ds_val['train'].batch(batch_size)\n",
    "\n",
    "model.eval()\n",
    "\n",
    "all_predictions = []\n",
    "all_logits = []\n",
    "\n",
    "\n",
    "with torch.no_grad():\n",
    "\n",
    "    for batch in tqdm.tqdm(ds_val, desc=\"Inference\"):\n",
    "        outputs = model(input_ids=torch.tensor(batch['input_ids']).to('cuda'), attention_mask=torch.tensor(batch['attention_mask']).to('cuda'))\n",
    "\n",
    "        logits = outputs.logits\n",
    "\n",
    "\n",
    "        predictions = torch.argmax(logits, dim=-1)\n",
    "        all_predictions.append(predictions.cpu())\n",
    "        all_logits.append(logits.cpu())\n",
    "\n",
    "final_predictions = torch.cat(all_predictions)\n",
    "final_logits = torch.cat(all_logits)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from data_loader import id2label\n",
    "verdicts = final_predictions.tolist()\n",
    "verdicts = [id2label[i] for i in verdicts]\n",
    "print(verdicts[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "def score(label, verdict):\n",
    "    if label == 'positive':\n",
    "        if verdict == 'positive':\n",
    "            return 1\n",
    "        elif verdict == 'neutral':\n",
    "            return 0.5\n",
    "        else:\n",
    "            return 0\n",
    "    elif label == 'negative':\n",
    "        if verdict == 'negative':\n",
    "            return 1\n",
    "        elif verdict == 'neutral':\n",
    "            return 0.5\n",
    "        else:\n",
    "            return 0\n",
    "    else:\n",
    "        if verdict == 'neutral':\n",
    "            return 1\n",
    "        elif verdict == 'positive':\n",
    "            return 0.5\n",
    "        else:\n",
    "            return 0.5\n",
    "        \n",
    "import random\n",
    "scores = []\n",
    "labels = val_dataset['label']\n",
    "# random.shuffle(labels)\n",
    "for i in range(len(labels)):\n",
    "    scores.append(score(labels[i], verdicts[i]))\n",
    "# get the mean of the scores\n",
    "mean_score = np.mean(scores)\n",
    "print(mean_score, np.std(scores))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from hydra import compose, initialize\n",
    "from omegaconf import OmegaConf\n",
    "from data_loader import get_dataset\n",
    "\n",
    "with initialize(version_base=None, config_path=\"config/classifier\", job_name=\"train\"):\n",
    "    cfg = compose(config_name=\"train\")\n",
    "\n",
    "\n",
    "cfg.data.path = './data/test'\n",
    "ds_test, collator = get_dataset(cfg, tokenizer=tokenizer)\n",
    "\n",
    "batch_size = 1\n",
    "ds_test = ds_test['train'].batch(batch_size)\n",
    "\n",
    "model.eval()\n",
    "\n",
    "all_predictions = []\n",
    "all_logits = []\n",
    "\n",
    "\n",
    "with torch.no_grad():\n",
    "\n",
    "    for batch in tqdm.tqdm(ds_test, desc=\"Inference\"):\n",
    "        outputs = model(input_ids=torch.tensor(batch['input_ids']).to('cuda'), attention_mask=torch.tensor(batch['attention_mask']).to('cuda'))\n",
    "\n",
    "        logits = outputs.logits\n",
    "\n",
    "\n",
    "        predictions = torch.argmax(logits, dim=-1)\n",
    "        all_predictions.append(predictions.cpu())\n",
    "        all_logits.append(logits.cpu())\n",
    "\n",
    "final_predictions = torch.cat(all_predictions)\n",
    "final_logits = torch.cat(all_logits)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from data_loader import id2label\n",
    "verdicts = final_predictions.tolist()\n",
    "verdicts = [id2label[i] for i in verdicts]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ids_ = ds_test['id']\n",
    "# We flatten the list of ds_test\n",
    "ids = []\n",
    "for i in range(len(ids_)):\n",
    "    ids.extend(ids_[i])\n",
    "print(ids[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "submission_df = pd.DataFrame({'id': ids, 'label': verdicts})\n",
    "submission_df.to_csv('submission.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_from_disk\n",
    "val_dataset = load_from_disk('data/validation')\n",
    "test_dataset = load_from_disk('data/test')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import xgboost as xgb\n",
    "\n",
    "def score(label, verdict):\n",
    "    if label == 'positive':\n",
    "        if verdict == 'positive':\n",
    "            return 1\n",
    "        elif verdict == 'neutral':\n",
    "            return 0.5\n",
    "        else:\n",
    "            return 0\n",
    "    elif label == 'negative':\n",
    "        if verdict == 'negative':\n",
    "            return 1\n",
    "        elif verdict == 'neutral':\n",
    "            return 0.5\n",
    "        else:\n",
    "            return 0\n",
    "    else:  # label is 'neutral'\n",
    "        if verdict == 'neutral':\n",
    "            return 1\n",
    "        elif verdict == 'positive':\n",
    "            return 0.5\n",
    "        else:\n",
    "            return 0.5\n",
    "\n",
    "def custom_objective(preds, dtrain):\n",
    "    num_class = 3\n",
    "    preds = preds.reshape(-1, num_class)\n",
    "\n",
    "    exp_preds = np.exp(preds - np.max(preds, axis=1, keepdims=True))\n",
    "    probs = exp_preds / np.sum(exp_preds, axis=1, keepdims=True)\n",
    "    \n",
    "    labels = dtrain.get_label().astype(int)\n",
    "    n = preds.shape[0]\n",
    "    \n",
    "    s = np.zeros_like(probs)\n",
    "    for i, label in enumerate(labels):\n",
    "        if label == 0:  # negative\n",
    "            s[i] = [1.0, 0.5, 0.0]\n",
    "        elif label == 1:  # neutral\n",
    "            s[i] = [0.5, 1.0, 0.5]\n",
    "        elif label == 2:  # positive\n",
    "            s[i] = [0.0, 0.5, 1.0]\n",
    "    \n",
    "    mu = np.sum(probs * s, axis=1, keepdims=True)  # shape (n, 1)\n",
    "    \n",
    "    grad = probs * (mu - s)\n",
    "\n",
    "    hess = probs * (mu - s) * (1 - 2 * probs)\n",
    "    hess = np.maximum(hess, 1e-6)\n",
    "    \n",
    "    return grad.flatten(), hess.flatten()\n",
    "\n",
    "from data_loader import id2label, label2id\n",
    "X_train = final_logits.float()\n",
    "y_train = val_dataset['label']\n",
    "y_train = [label2id[i] for i in y_train]\n",
    "\n",
    "X_train_np = X_train.detach().cpu().numpy()  # training features\n",
    "X_val_np   = X_train.detach().cpu().numpy()      # validation features\n",
    "\n",
    "# Convert labels (already processed via label2id) to numpy arrays.\n",
    "y_train_np = np.array(y_train)\n",
    "y_val_np   = np.array(y_train)\n",
    "\n",
    "# Prepare DMatrix for training and validation.\n",
    "dtrain = xgb.DMatrix(X_train_np, label=y_train_np)\n",
    "dval   = xgb.DMatrix(X_val_np, label=y_val_np)\n",
    "\n",
    "params = {\n",
    "    'num_class': 3,\n",
    "    'eval_metric': 'mlogloss'  # this metric is used for logging; training uses the custom objective\n",
    "}\n",
    "\n",
    "# Create a watchlist to monitor performance.\n",
    "watchlist = [(dtrain, 'train'), (dval, 'eval')]\n",
    "\n",
    "# Train the model using xgb.train with the custom objective.\n",
    "bst = xgb.train(params, dtrain, num_boost_round=10000, obj=custom_objective, evals=watchlist)\n",
    "\n",
    "# -------------------------------\n",
    "# Evaluation using your custom scoring function.\n",
    "# First, get predictions on the validation set.\n",
    "# Note: With a custom objective, predictions are still raw scores.\n",
    "# We reshape them to (n_samples, num_class) and take argmax to get the predicted label.\n",
    "preds = bst.predict(dval)\n",
    "print(preds)\n",
    "pred_labels = preds\n",
    "print(y_val_np, pred_labels)\n",
    "# Mapping from indices to verdict strings (order must match the one used above).\n",
    "verdicts = ['negative', 'neutral', 'positive']\n",
    "\n",
    "# Calculate custom scores for each instance.\n",
    "scores = [score(verdicts[int(y_true)], verdicts[int(y_pred)])\n",
    "          for y_true, y_pred in zip(y_val_np, pred_labels)]\n",
    "\n",
    "mean_score = np.mean(scores)\n",
    "std_score = np.std(scores)\n",
    "print(\"Mean custom score:\", mean_score)\n",
    "print(\"Std custom score:\", std_score)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
