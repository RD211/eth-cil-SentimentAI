{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import Dataset\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df = pd.read_csv('data/test.csv')\n",
    "train_df = pd.read_csv('data/training.csv')\n",
    "train_df, test_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = Dataset.from_dict({'text': train_df['sentence'], 'label': train_df['label']})\n",
    "test_dataset = Dataset.from_dict({'id': test_df['id'], 'text': test_df['sentence']})\n",
    "# shuffle\n",
    "train_dataset = train_dataset.shuffle()\n",
    "\n",
    "validation_dataset = train_dataset.train_test_split(test_size=0.1)\n",
    "train_dataset = validation_dataset['train']\n",
    "validation_dataset = validation_dataset['test']\n",
    "# Save the datasets to disk\n",
    "train_dataset.save_to_disk('data/train')\n",
    "test_dataset.save_to_disk('data/test')\n",
    "validation_dataset.save_to_disk('data/validation')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"Qwen/Qwen2.5-0.5B-Instruct\")\n",
    "\n",
    "train_dataset = train_dataset.map(\n",
    "    lambda examples: {\"len\":len(tokenizer.encode(\n",
    "        examples['text'],\n",
    "    )),\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get max length\n",
    "max_length = max(train_dataset['len'])\n",
    "print(max_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from vllm import LLM, SamplingParams\n",
    "\n",
    "llm = LLM(model=\"checkpoints/0.5b-rl/checkpoint-620\", max_model_len=1500, gpu_memory_utilization=0.8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of LlamaForSequenceClassification were not initialized from the model checkpoint at HuggingFaceTB/SmolLM2-360M and are newly initialized: ['score.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LlamaForSequenceClassification(\n",
      "  (model): LlamaModel(\n",
      "    (embed_tokens): Embedding(49152, 960)\n",
      "    (layers): ModuleList(\n",
      "      (0-31): 32 x LlamaDecoderLayer(\n",
      "        (self_attn): LlamaAttention(\n",
      "          (q_proj): Linear(in_features=960, out_features=960, bias=False)\n",
      "          (k_proj): Linear(in_features=960, out_features=320, bias=False)\n",
      "          (v_proj): Linear(in_features=960, out_features=320, bias=False)\n",
      "          (o_proj): Linear(in_features=960, out_features=960, bias=False)\n",
      "        )\n",
      "        (mlp): LlamaMLP(\n",
      "          (gate_proj): Linear(in_features=960, out_features=2560, bias=False)\n",
      "          (up_proj): Linear(in_features=960, out_features=2560, bias=False)\n",
      "          (down_proj): Linear(in_features=2560, out_features=960, bias=False)\n",
      "          (act_fn): SiLU()\n",
      "        )\n",
      "        (input_layernorm): LlamaRMSNorm((960,), eps=1e-05)\n",
      "        (post_attention_layernorm): LlamaRMSNorm((960,), eps=1e-05)\n",
      "      )\n",
      "    )\n",
      "    (norm): LlamaRMSNorm((960,), eps=1e-05)\n",
      "    (rotary_emb): LlamaRotaryEmbedding()\n",
      "  )\n",
      "  (score): Linear(in_features=960, out_features=3, bias=False)\n",
      ")\n",
      "Loading llm weights for head initialization.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Linear(in_features=960, out_features=3, bias=False)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoModelForSequenceClassification, AutoTokenizer\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\n",
    "    \"HuggingFaceTB/SmolLM2-360M\",\n",
    "    padding_side=\"left\",\n",
    "    use_fast=True,\n",
    ")\n",
    "model = AutoModelForSequenceClassification.from_pretrained(\n",
    "    \"HuggingFaceTB/SmolLM2-360M\",\n",
    "    num_labels=3,\n",
    "    torch_dtype=torch.bfloat16,\n",
    "\n",
    ").to(\"cuda\")\n",
    "model.config.use_cache = False\n",
    "model.config.pad_token_id = tokenizer.pad_token_id\n",
    "print(model)\n",
    "print(\"Loading llm weights for head initialization.\")\n",
    "llm = AutoModelForCausalLM.from_pretrained(\n",
    "    \"HuggingFaceTB/SmolLM2-360M\",\n",
    "    torch_dtype=torch.bfloat16,\n",
    ").to(\"cuda\")\n",
    "label2id = {\n",
    "    \"negative\": 0,\n",
    "    \"neutral\": 1,\n",
    "    \"positive\": 2\n",
    "}\n",
    "classes = list(label2id.keys())\n",
    "tokens = [tokenizer.encode(c)[-1] for c in classes]\n",
    "\n",
    "model.score.weight = nn.Parameter(llm.lm_head.weight[tokens].clone())\n",
    "# print(\"Updated weights of model with lm_head.\")\n",
    "# del llm\n",
    "model.score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SequenceClassifierOutputWithPast(loss=None, logits=tensor([[1.8594, 0.2676, 2.1875]], device='cuda:0', dtype=torch.bfloat16,\n",
       "       grad_fn=<IndexBackward0>), past_key_values=None, hidden_states=None, attentions=None)"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model(input_ids=torch.tensor([tokenizer.encode(\"I love you\")]).to(\"cuda\"),\n",
    "      )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_from_disk\n",
    "val_dataset = load_from_disk('data/validation')\n",
    "test_dataset = load_from_disk('data/test')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sampling_params = SamplingParams(\n",
    "    n = 1,\n",
    "    temperature=1.0,\n",
    "    top_p=0.9,\n",
    "    max_tokens=1500,\n",
    ")\n",
    "tokenizer = llm.get_tokenizer()\n",
    "prompt = open(\"prompt_templates/sentiment.txt\", \"r\").read()\n",
    "val_dataset = val_dataset.map(lambda conversation: {\"conv\": tokenizer.apply_chat_template([\n",
    "    {\"role\": \"user\", \"content\": prompt.format(conversation['text'])}\n",
    "    ], tokenize=False, add_generation_prompt=True)\n",
    "}\n",
    ")\n",
    "\n",
    "to_process = val_dataset['conv']\n",
    "outputs = llm.generate(\n",
    "    to_process,\n",
    "    sampling_params=sampling_params,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "verdicts = []\n",
    "for i in range(len(outputs)):\n",
    "  type_of_verdicts = ['boxed{negative}', 'boxed{positive}', 'boxed{neutral}']\n",
    "  texts = [outputs[i].outputs[j].text for j in range(len(outputs[i].outputs))]\n",
    "  # We do a majority vote to get the final verdict\n",
    "  verdict = max(type_of_verdicts, key=lambda x: sum([1 for text in texts if x in text]))\n",
    "  verdicts.append(verdict.replace('boxed{', '').replace('}', ''))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "def score(label, verdict):\n",
    "    if label == 'positive':\n",
    "        if verdict == 'positive':\n",
    "            return 1\n",
    "        elif verdict == 'neutral':\n",
    "            return 0.5\n",
    "        else:\n",
    "            return 0\n",
    "    elif label == 'negative':\n",
    "        if verdict == 'negative':\n",
    "            return 1\n",
    "        elif verdict == 'neutral':\n",
    "            return 0.5\n",
    "        else:\n",
    "            return 0\n",
    "    else:\n",
    "        if verdict == 'neutral':\n",
    "            return 1\n",
    "        elif verdict == 'positive':\n",
    "            return 0.5\n",
    "        else:\n",
    "            return 0.5\n",
    "scores = []\n",
    "labels = val_dataset['label']\n",
    "for i in range(len(labels)):\n",
    "    scores.append(score(labels[i], verdicts[i]))\n",
    "# get the mean of the scores\n",
    "mean_score = np.mean(scores)\n",
    "print(mean_score, np.std(scores))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sampling_params = SamplingParams(\n",
    "    n = 1,\n",
    "    temperature=1.0,\n",
    "    top_p=0.9,\n",
    "    max_tokens=1500,\n",
    ")\n",
    "tokenizer = llm.get_tokenizer()\n",
    "prompt = open(\"prompt_templates/sentiment.txt\", \"r\").read()\n",
    "test_dataset = test_dataset.map(lambda conversation: {\"conv\": tokenizer.apply_chat_template([\n",
    "    {\"role\": \"user\", \"content\": prompt.format(conversation['text'])}\n",
    "    ], tokenize=False, add_generation_prompt=True)\n",
    "}\n",
    ")\n",
    "\n",
    "to_process = test_dataset['conv']\n",
    "outputs = llm.generate(\n",
    "    to_process,\n",
    "    sampling_params=sampling_params,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "verdicts = []\n",
    "for i in range(len(outputs)):\n",
    "  type_of_verdicts = ['boxed{negative}', 'boxed{positive}', 'boxed{neutral}']\n",
    "  texts = [outputs[i].outputs[j].text for j in range(len(outputs[i].outputs))]\n",
    "  # We do a majority vote to get the final verdict\n",
    "  verdict = max(type_of_verdicts, key=lambda x: sum([1 for text in texts if x in text]))\n",
    "  verdicts.append(verdict.replace('boxed{', '').replace('}', ''))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save verdicts to file\n",
    "import pandas as pd\n",
    "submission_df = pd.DataFrame({'id': test_dataset['id'], 'label': verdicts})\n",
    "submission_df.to_csv('submission.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "import tqdm\n",
    "\n",
    "model = AutoModelForSequenceClassification.from_pretrained(\"rd211/custom-trainer\", torch_dtype=torch.bfloat16, device_map=\"auto\", num_labels=3)\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"rd211/custom-trainer\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from hydra import compose, initialize\n",
    "from omegaconf import OmegaConf\n",
    "from data_loader import get_dataset\n",
    "\n",
    "with initialize(version_base=None, config_path=\"config/classifier\", job_name=\"train\"):\n",
    "    cfg = compose(config_name=\"train\")\n",
    "\n",
    "\n",
    "cfg.data.path = './data/validation'\n",
    "ds_val, collator = get_dataset(cfg, tokenizer=tokenizer)\n",
    "\n",
    "batch_size = 1\n",
    "ds_val = ds_val['train'].batch(batch_size)\n",
    "\n",
    "model.eval()\n",
    "\n",
    "all_predictions = []\n",
    "all_logits = []\n",
    "\n",
    "\n",
    "with torch.no_grad():\n",
    "\n",
    "    for batch in tqdm.tqdm(ds_val, desc=\"Inference\"):\n",
    "        outputs = model(input_ids=torch.tensor(batch['input_ids']).to('cuda'), attention_mask=torch.tensor(batch['attention_mask']).to('cuda'))\n",
    "\n",
    "        logits = outputs.logits\n",
    "\n",
    "\n",
    "        predictions = torch.argmax(logits, dim=-1)\n",
    "        all_predictions.append(predictions.cpu())\n",
    "        all_logits.append(logits.cpu())\n",
    "\n",
    "final_predictions = torch.cat(all_predictions)\n",
    "final_logits = torch.cat(all_logits)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from data_loader import id2label\n",
    "verdicts = final_predictions.tolist()\n",
    "verdicts = [id2label[i] for i in verdicts]\n",
    "print(verdicts[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "def score(label, verdict):\n",
    "    if label == 'positive':\n",
    "        if verdict == 'positive':\n",
    "            return 1\n",
    "        elif verdict == 'neutral':\n",
    "            return 0.5\n",
    "        else:\n",
    "            return 0\n",
    "    elif label == 'negative':\n",
    "        if verdict == 'negative':\n",
    "            return 1\n",
    "        elif verdict == 'neutral':\n",
    "            return 0.5\n",
    "        else:\n",
    "            return 0\n",
    "    else:\n",
    "        if verdict == 'neutral':\n",
    "            return 1\n",
    "        elif verdict == 'positive':\n",
    "            return 0.5\n",
    "        else:\n",
    "            return 0.5\n",
    "        \n",
    "import random\n",
    "scores = []\n",
    "labels = val_dataset['label']\n",
    "# random.shuffle(labels)\n",
    "for i in range(len(labels)):\n",
    "    scores.append(score(labels[i], verdicts[i]))\n",
    "# get the mean of the scores\n",
    "mean_score = np.mean(scores)\n",
    "print(mean_score, np.std(scores))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from hydra import compose, initialize\n",
    "from omegaconf import OmegaConf\n",
    "from data_loader import get_dataset\n",
    "\n",
    "with initialize(version_base=None, config_path=\"config/classifier\", job_name=\"train\"):\n",
    "    cfg = compose(config_name=\"train\")\n",
    "\n",
    "\n",
    "cfg.data.path = './data/test'\n",
    "ds_test, collator = get_dataset(cfg, tokenizer=tokenizer)\n",
    "\n",
    "batch_size = 1\n",
    "ds_test = ds_test['train'].batch(batch_size)\n",
    "\n",
    "model.eval()\n",
    "\n",
    "all_predictions = []\n",
    "all_logits = []\n",
    "\n",
    "\n",
    "with torch.no_grad():\n",
    "\n",
    "    for batch in tqdm.tqdm(ds_test, desc=\"Inference\"):\n",
    "        outputs = model(input_ids=torch.tensor(batch['input_ids']).to('cuda'), attention_mask=torch.tensor(batch['attention_mask']).to('cuda'))\n",
    "\n",
    "        logits = outputs.logits\n",
    "\n",
    "\n",
    "        predictions = torch.argmax(logits, dim=-1)\n",
    "        all_predictions.append(predictions.cpu())\n",
    "        all_logits.append(logits.cpu())\n",
    "\n",
    "final_predictions = torch.cat(all_predictions)\n",
    "final_logits = torch.cat(all_logits)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from data_loader import id2label\n",
    "verdicts = final_predictions.tolist()\n",
    "verdicts = [id2label[i] for i in verdicts]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ids_ = ds_test['id']\n",
    "# We flatten the list of ds_test\n",
    "ids = []\n",
    "for i in range(len(ids_)):\n",
    "    ids.extend(ids_[i])\n",
    "print(ids[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "submission_df = pd.DataFrame({'id': ids, 'label': verdicts})\n",
    "submission_df.to_csv('submission.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_from_disk\n",
    "val_dataset = load_from_disk('data/validation')\n",
    "test_dataset = load_from_disk('data/test')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from rag import EmbeddingStore\n",
    "\n",
    "from datasets import load_from_disk\n",
    "val_dataset = load_from_disk('data/validation')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['text', 'label'],\n",
       "    num_rows: 5105\n",
       "})"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-05-23 05:00:35,710] [INFO] [real_accelerator.py:219:get_accelerator] Setting ds_accelerator to cuda (auto detect)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/rd/miniconda3/compiler_compat/ld: cannot find -laio: No such file or directory\n",
      "collect2: error: ld returned 1 exit status\n",
      "/home/rd/miniconda3/compiler_compat/ld: cannot find -lcufile: No such file or directory\n",
      "collect2: error: ld returned 1 exit status\n",
      "/home/rd/miniconda3/lib/python3.12/site-packages/flash_attn/ops/triton/layer_norm.py:984: FutureWarning: `torch.cuda.amp.custom_fwd(args...)` is deprecated. Please use `torch.amp.custom_fwd(args..., device_type='cuda')` instead.\n",
      "  @custom_fwd\n",
      "/home/rd/miniconda3/lib/python3.12/site-packages/flash_attn/ops/triton/layer_norm.py:1043: FutureWarning: `torch.cuda.amp.custom_bwd(args...)` is deprecated. Please use `torch.amp.custom_bwd(args..., device_type='cuda')` instead.\n",
      "  @custom_bwd\n"
     ]
    }
   ],
   "source": [
    "embedding_store = EmbeddingStore(\n",
    "  embedding_model='jinaai/jina-embeddings-v3',\n",
    "  ds = val_dataset,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'review': 'Look forward to seeing you.', 'classification': 'positive'},\n",
       " {'review': 'Right...', 'classification': 'neutral'},\n",
       " {'review': 'What?', 'classification': 'neutral'},\n",
       " {'review': 'Right!', 'classification': 'neutral'},\n",
       " {'review': 'Please come here.', 'classification': 'positive'}]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embedding_store.get_k_nearest(\"Hello world\", k=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from jinja2 import Template\n",
    "template = Template(open(\"prompt_templates/sentiment_kshot.txt\", \"r\").read())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'You are tasked with indetifying if a certain review is negative, neutral or positive.\\n\\n\\nReview: Look forward to seeing you.\\nClassification: positive\\n\\nReview: Right...\\nClassification: neutral\\n\\nReview: What?\\nClassification: neutral\\n\\nReview: Right!\\nClassification: neutral\\n\\nReview: Please come here.\\nClassification: positive\\n\\nReview: \\nClassification:'"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "template.render(\n",
    "  examples=embedding_store.get_k_nearest(\"Hello world\", k=5),\n",
    "  query=\"Hello world\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
