{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import Dataset\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df = pd.read_csv('data/test.csv')\n",
    "train_df = pd.read_csv('data/training.csv')\n",
    "train_df, test_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = Dataset.from_dict({'text': train_df['sentence'], 'label': train_df['label']})\n",
    "test_dataset = Dataset.from_dict({'id': test_df['id'], 'text': test_df['sentence']})\n",
    "# shuffle\n",
    "train_dataset = train_dataset.shuffle()\n",
    "\n",
    "validation_dataset = train_dataset.train_test_split(test_size=0.1)\n",
    "train_dataset = validation_dataset['train']\n",
    "validation_dataset = validation_dataset['test']\n",
    "# Save the datasets to disk\n",
    "train_dataset.save_to_disk('data/train')\n",
    "test_dataset.save_to_disk('data/test')\n",
    "validation_dataset.save_to_disk('data/validation')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"Qwen/Qwen2.5-0.5B-Instruct\")\n",
    "\n",
    "train_dataset = train_dataset.map(\n",
    "    lambda examples: {\"len\":len(tokenizer.encode(\n",
    "        examples['text'],\n",
    "    )),\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get max length\n",
    "max_length = max(train_dataset['len'])\n",
    "print(max_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from vllm import LLM, SamplingParams\n",
    "\n",
    "llm = LLM(model=\"checkpoints/0.5b-rl/checkpoint-620\", max_model_len=1500, gpu_memory_utilization=0.8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_from_disk\n",
    "val_dataset = load_from_disk('data/validation')\n",
    "test_dataset = load_from_disk('data/test')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sampling_params = SamplingParams(\n",
    "    n = 1,\n",
    "    temperature=1.0,\n",
    "    top_p=0.9,\n",
    "    max_tokens=1500,\n",
    ")\n",
    "tokenizer = llm.get_tokenizer()\n",
    "prompt = open(\"prompt_templates/sentiment.txt\", \"r\").read()\n",
    "val_dataset = val_dataset.map(lambda conversation: {\"conv\": tokenizer.apply_chat_template([\n",
    "    {\"role\": \"user\", \"content\": prompt.format(conversation['text'])}\n",
    "    ], tokenize=False, add_generation_prompt=True)\n",
    "}\n",
    ")\n",
    "\n",
    "to_process = val_dataset['conv']\n",
    "outputs = llm.generate(\n",
    "    to_process,\n",
    "    sampling_params=sampling_params,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "verdicts = []\n",
    "for i in range(len(outputs)):\n",
    "  type_of_verdicts = ['boxed{negative}', 'boxed{positive}', 'boxed{neutral}']\n",
    "  texts = [outputs[i].outputs[j].text for j in range(len(outputs[i].outputs))]\n",
    "  # We do a majority vote to get the final verdict\n",
    "  verdict = max(type_of_verdicts, key=lambda x: sum([1 for text in texts if x in text]))\n",
    "  verdicts.append(verdict.replace('boxed{', '').replace('}', ''))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "def score(label, verdict):\n",
    "    if label == 'positive':\n",
    "        if verdict == 'positive':\n",
    "            return 1\n",
    "        elif verdict == 'neutral':\n",
    "            return 0.5\n",
    "        else:\n",
    "            return 0\n",
    "    elif label == 'negative':\n",
    "        if verdict == 'negative':\n",
    "            return 1\n",
    "        elif verdict == 'neutral':\n",
    "            return 0.5\n",
    "        else:\n",
    "            return 0\n",
    "    else:\n",
    "        if verdict == 'neutral':\n",
    "            return 1\n",
    "        elif verdict == 'positive':\n",
    "            return 0.5\n",
    "        else:\n",
    "            return 0.5\n",
    "scores = []\n",
    "labels = val_dataset['label']\n",
    "for i in range(len(labels)):\n",
    "    scores.append(score(labels[i], verdicts[i]))\n",
    "# get the mean of the scores\n",
    "mean_score = np.mean(scores)\n",
    "print(mean_score, np.std(scores))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sampling_params = SamplingParams(\n",
    "    n = 1,\n",
    "    temperature=1.0,\n",
    "    top_p=0.9,\n",
    "    max_tokens=1500,\n",
    ")\n",
    "tokenizer = llm.get_tokenizer()\n",
    "prompt = open(\"prompt_templates/sentiment.txt\", \"r\").read()\n",
    "test_dataset = test_dataset.map(lambda conversation: {\"conv\": tokenizer.apply_chat_template([\n",
    "    {\"role\": \"user\", \"content\": prompt.format(conversation['text'])}\n",
    "    ], tokenize=False, add_generation_prompt=True)\n",
    "}\n",
    ")\n",
    "\n",
    "to_process = test_dataset['conv']\n",
    "outputs = llm.generate(\n",
    "    to_process,\n",
    "    sampling_params=sampling_params,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "verdicts = []\n",
    "for i in range(len(outputs)):\n",
    "  type_of_verdicts = ['boxed{negative}', 'boxed{positive}', 'boxed{neutral}']\n",
    "  texts = [outputs[i].outputs[j].text for j in range(len(outputs[i].outputs))]\n",
    "  # We do a majority vote to get the final verdict\n",
    "  verdict = max(type_of_verdicts, key=lambda x: sum([1 for text in texts if x in text]))\n",
    "  verdicts.append(verdict.replace('boxed{', '').replace('}', ''))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save verdicts to file\n",
    "import pandas as pd\n",
    "submission_df = pd.DataFrame({'id': test_dataset['id'], 'label': verdicts})\n",
    "submission_df.to_csv('submission.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "import tqdm\n",
    "\n",
    "model = AutoModelForSequenceClassification.from_pretrained(\"rd211/custom-trainer\", torch_dtype=torch.bfloat16, device_map=\"auto\", num_labels=3)\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"rd211/custom-trainer\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from hydra import compose, initialize\n",
    "from omegaconf import OmegaConf\n",
    "from data_loader import get_dataset\n",
    "\n",
    "with initialize(version_base=None, config_path=\"config/classifier\", job_name=\"train\"):\n",
    "    cfg = compose(config_name=\"train\")\n",
    "\n",
    "\n",
    "cfg.data.path = './data/validation'\n",
    "ds_val, collator = get_dataset(cfg, tokenizer=tokenizer)\n",
    "\n",
    "batch_size = 1\n",
    "ds_val = ds_val['train'].batch(batch_size)\n",
    "\n",
    "model.eval()\n",
    "\n",
    "all_predictions = []\n",
    "all_logits = []\n",
    "\n",
    "\n",
    "with torch.no_grad():\n",
    "\n",
    "    for batch in tqdm.tqdm(ds_val, desc=\"Inference\"):\n",
    "        outputs = model(input_ids=torch.tensor(batch['input_ids']).to('cuda'), attention_mask=torch.tensor(batch['attention_mask']).to('cuda'))\n",
    "\n",
    "        logits = outputs.logits\n",
    "\n",
    "\n",
    "        predictions = torch.argmax(logits, dim=-1)\n",
    "        all_predictions.append(predictions.cpu())\n",
    "        all_logits.append(logits.cpu())\n",
    "\n",
    "final_predictions = torch.cat(all_predictions)\n",
    "final_logits = torch.cat(all_logits)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from data_loader import id2label\n",
    "verdicts = final_predictions.tolist()\n",
    "verdicts = [id2label[i] for i in verdicts]\n",
    "print(verdicts[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "def score(label, verdict):\n",
    "    if label == 'positive':\n",
    "        if verdict == 'positive':\n",
    "            return 1\n",
    "        elif verdict == 'neutral':\n",
    "            return 0.5\n",
    "        else:\n",
    "            return 0\n",
    "    elif label == 'negative':\n",
    "        if verdict == 'negative':\n",
    "            return 1\n",
    "        elif verdict == 'neutral':\n",
    "            return 0.5\n",
    "        else:\n",
    "            return 0\n",
    "    else:\n",
    "        if verdict == 'neutral':\n",
    "            return 1\n",
    "        elif verdict == 'positive':\n",
    "            return 0.5\n",
    "        else:\n",
    "            return 0.5\n",
    "        \n",
    "import random\n",
    "scores = []\n",
    "labels = val_dataset['label']\n",
    "# random.shuffle(labels)\n",
    "for i in range(len(labels)):\n",
    "    scores.append(score(labels[i], verdicts[i]))\n",
    "# get the mean of the scores\n",
    "mean_score = np.mean(scores)\n",
    "print(mean_score, np.std(scores))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from hydra import compose, initialize\n",
    "from omegaconf import OmegaConf\n",
    "from data_loader import get_dataset\n",
    "\n",
    "with initialize(version_base=None, config_path=\"config/classifier\", job_name=\"train\"):\n",
    "    cfg = compose(config_name=\"train\")\n",
    "\n",
    "\n",
    "cfg.data.path = './data/test'\n",
    "ds_test, collator = get_dataset(cfg, tokenizer=tokenizer)\n",
    "\n",
    "batch_size = 1\n",
    "ds_test = ds_test['train'].batch(batch_size)\n",
    "\n",
    "model.eval()\n",
    "\n",
    "all_predictions = []\n",
    "all_logits = []\n",
    "\n",
    "\n",
    "with torch.no_grad():\n",
    "\n",
    "    for batch in tqdm.tqdm(ds_test, desc=\"Inference\"):\n",
    "        outputs = model(input_ids=torch.tensor(batch['input_ids']).to('cuda'), attention_mask=torch.tensor(batch['attention_mask']).to('cuda'))\n",
    "\n",
    "        logits = outputs.logits\n",
    "\n",
    "\n",
    "        predictions = torch.argmax(logits, dim=-1)\n",
    "        all_predictions.append(predictions.cpu())\n",
    "        all_logits.append(logits.cpu())\n",
    "\n",
    "final_predictions = torch.cat(all_predictions)\n",
    "final_logits = torch.cat(all_logits)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from data_loader import id2label\n",
    "verdicts = final_predictions.tolist()\n",
    "verdicts = [id2label[i] for i in verdicts]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ids_ = ds_test['id']\n",
    "# We flatten the list of ds_test\n",
    "ids = []\n",
    "for i in range(len(ids_)):\n",
    "    ids.extend(ids_[i])\n",
    "print(ids[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "submission_df = pd.DataFrame({'id': ids, 'label': verdicts})\n",
    "submission_df.to_csv('submission.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_from_disk\n",
    "val_dataset = load_from_disk('data/validation')\n",
    "test_dataset = load_from_disk('data/test')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import xgboost as xgb\n",
    "\n",
    "def score(label, verdict):\n",
    "    if label == 'positive':\n",
    "        if verdict == 'positive':\n",
    "            return 1\n",
    "        elif verdict == 'neutral':\n",
    "            return 0.5\n",
    "        else:\n",
    "            return 0\n",
    "    elif label == 'negative':\n",
    "        if verdict == 'negative':\n",
    "            return 1\n",
    "        elif verdict == 'neutral':\n",
    "            return 0.5\n",
    "        else:\n",
    "            return 0\n",
    "    else:  # label is 'neutral'\n",
    "        if verdict == 'neutral':\n",
    "            return 1\n",
    "        elif verdict == 'positive':\n",
    "            return 0.5\n",
    "        else:\n",
    "            return 0.5\n",
    "\n",
    "def custom_objective(preds, dtrain):\n",
    "    num_class = 3\n",
    "    preds = preds.reshape(-1, num_class)\n",
    "\n",
    "    exp_preds = np.exp(preds - np.max(preds, axis=1, keepdims=True))\n",
    "    probs = exp_preds / np.sum(exp_preds, axis=1, keepdims=True)\n",
    "    \n",
    "    labels = dtrain.get_label().astype(int)\n",
    "    n = preds.shape[0]\n",
    "    \n",
    "    s = np.zeros_like(probs)\n",
    "    for i, label in enumerate(labels):\n",
    "        if label == 0:  # negative\n",
    "            s[i] = [1.0, 0.5, 0.0]\n",
    "        elif label == 1:  # neutral\n",
    "            s[i] = [0.5, 1.0, 0.5]\n",
    "        elif label == 2:  # positive\n",
    "            s[i] = [0.0, 0.5, 1.0]\n",
    "    \n",
    "    mu = np.sum(probs * s, axis=1, keepdims=True)  # shape (n, 1)\n",
    "    \n",
    "    grad = probs * (mu - s)\n",
    "\n",
    "    hess = probs * (mu - s) * (1 - 2 * probs)\n",
    "    hess = np.maximum(hess, 1e-6)\n",
    "    \n",
    "    return grad.flatten(), hess.flatten()\n",
    "\n",
    "from data_loader import id2label, label2id\n",
    "X_train = final_logits.float()\n",
    "y_train = val_dataset['label']\n",
    "y_train = [label2id[i] for i in y_train]\n",
    "\n",
    "X_train_np = X_train.detach().cpu().numpy()  # training features\n",
    "X_val_np   = X_train.detach().cpu().numpy()      # validation features\n",
    "\n",
    "# Convert labels (already processed via label2id) to numpy arrays.\n",
    "y_train_np = np.array(y_train)\n",
    "y_val_np   = np.array(y_train)\n",
    "\n",
    "# Prepare DMatrix for training and validation.\n",
    "dtrain = xgb.DMatrix(X_train_np, label=y_train_np)\n",
    "dval   = xgb.DMatrix(X_val_np, label=y_val_np)\n",
    "\n",
    "params = {\n",
    "    'num_class': 3,\n",
    "    'eval_metric': 'mlogloss'  # this metric is used for logging; training uses the custom objective\n",
    "}\n",
    "\n",
    "# Create a watchlist to monitor performance.\n",
    "watchlist = [(dtrain, 'train'), (dval, 'eval')]\n",
    "\n",
    "# Train the model using xgb.train with the custom objective.\n",
    "bst = xgb.train(params, dtrain, num_boost_round=10000, obj=custom_objective, evals=watchlist)\n",
    "\n",
    "# -------------------------------\n",
    "# Evaluation using your custom scoring function.\n",
    "# First, get predictions on the validation set.\n",
    "# Note: With a custom objective, predictions are still raw scores.\n",
    "# We reshape them to (n_samples, num_class) and take argmax to get the predicted label.\n",
    "preds = bst.predict(dval)\n",
    "print(preds)\n",
    "pred_labels = preds\n",
    "print(y_val_np, pred_labels)\n",
    "# Mapping from indices to verdict strings (order must match the one used above).\n",
    "verdicts = ['negative', 'neutral', 'positive']\n",
    "\n",
    "# Calculate custom scores for each instance.\n",
    "scores = [score(verdicts[int(y_true)], verdicts[int(y_pred)])\n",
    "          for y_true, y_pred in zip(y_val_np, pred_labels)]\n",
    "\n",
    "mean_score = np.mean(scores)\n",
    "std_score = np.std(scores)\n",
    "print(\"Mean custom score:\", mean_score)\n",
    "print(\"Std custom score:\", std_score)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
